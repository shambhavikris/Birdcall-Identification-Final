{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1416,
     "status": "ok",
     "timestamp": 1599414689293,
     "user": {
      "displayName": "ShambSar",
      "photoUrl": "",
      "userId": "01685406212133283221"
     },
     "user_tz": -330
    },
    "id": "UnajK5UNg2oP"
   },
   "outputs": [],
   "source": [
    "# each TF-Record file is for a single class and will be loaded into a separate tf.data.Dataset\n",
    "# these datasets will be appended to a list, and fed into tf.experimental.sample_from_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5295,
     "status": "ok",
     "timestamp": 1599414693178,
     "user": {
      "displayName": "ShambSar",
      "photoUrl": "",
      "userId": "01685406212133283221"
     },
     "user_tz": -330
    },
    "id": "1CydPQMnh3ac"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pathlib\n",
    "import gc\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a6XK3HueOeNs"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QRfgh98I3eUZ"
   },
   "outputs": [],
   "source": [
    "N_CLASSES = 264 \n",
    "SAMPLE_RATE = 30000 # Audio sample rate\n",
    "MAX_DURATION = 5 # Clip duration in seconds \n",
    "FFT_SIZE = 1024 # Fourier Transform size \n",
    "HOP_SIZE = 512 # Number of samples between each successive FFT window\n",
    "N_MEL_BINS = 128 \n",
    "N_SPECTROGRAM_BINS = (FFT_SIZE // 2) + 1\n",
    "F_MIN = 20 # Min frequency cutoff\n",
    "F_MAX = SAMPLE_RATE / 2  # Max Frequency cutoff\n",
    "BATCH_SIZE = 16  # Training Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cYJdCaKEFRlc"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/content/drive/My Drive/train.csv\", parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O9czwVPJOKTN"
   },
   "outputs": [],
   "source": [
    "# directories for Train and Test TF Records, 264 files in each\n",
    "train_dir = '/content/drive/My Drive/lala1/Data-5Seconds/Train'\n",
    "test_dir = '/content/drive/My Drive/lala1/Data-5Seconds/Test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook uses 5 seconds worth of data at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hzz6t3TfhTYV"
   },
   "outputs": [],
   "source": [
    "train_tfr = os.listdir(train_dir)\n",
    "test_tfr = os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIQteKtFhk8A"
   },
   "outputs": [],
   "source": [
    "def read_tfrecord(serialized_example):\n",
    "    feature_description = {\n",
    "          'feature0': tf.io.FixedLenFeature((), tf.string),\n",
    "          'feature1': tf.io.FixedLenFeature((), tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "\n",
    "    feature0 = tf.io.parse_tensor(example['feature0'], out_type = tf.float32)\n",
    "    feature1 = example['feature1']\n",
    "\n",
    "    return feature0, feature1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "66NKzhqchsSL"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "lis = [0]*264\n",
    "for d in train_tfr:\n",
    "    d_path = os.path.join(train_dir, d)\n",
    "    tfrecord_dataset_train = tf.data.TFRecordDataset([d_path], compression_type=\"GZIP\")\n",
    "    dataset = tfrecord_dataset_train.map(read_tfrecord)\n",
    "    lis[i] = dataset\n",
    "    i = i+1\n",
    "\n",
    "train_ds = tf.data.experimental.sample_from_datasets(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_terMwZpOZah"
   },
   "outputs": [],
   "source": [
    "print(i) #264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trgNh8APig7s"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "lis = [0]*264\n",
    "for d in test_tfr:\n",
    "    d_path = os.path.join(test_dir, d)\n",
    "    tfrecord_dataset_test = tf.data.TFRecordDataset([d_path], compression_type=\"GZIP\")\n",
    "    dataset = tfrecord_dataset_test.map(read_tfrecord)\n",
    "    lis[i] = dataset\n",
    "    i = i+1\n",
    "\n",
    "test_ds = tf.data.experimental.sample_from_datasets(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7JW3BYuANfri"
   },
   "outputs": [],
   "source": [
    "print(i) #264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TDhK_drviwyL"
   },
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, shuffle_buffer_size=64, batch_size=24):\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    ds = ds.batch(batch_size).repeat()\n",
    "    ds = ds.map(setshape)\n",
    "    ds = ds.prefetch(2)\n",
    "    return ds\n",
    "\n",
    "def setshape(x, y):\n",
    "    x.set_shape([None, 150000,1])\n",
    "    y = tf.expand_dims(y, -1)\n",
    "    y.set_shape([None,1])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymrkwibnizvX"
   },
   "outputs": [],
   "source": [
    "train_final = prepare_for_training(train_ds)\n",
    "test_final = prepare_for_training(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearing RAM space by deleting the variables no longer needed, and calling the garbage collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qY_9X49vQCR3"
   },
   "outputs": [],
   "source": [
    "del train_dir\n",
    "del test_dir\n",
    "del d\n",
    "del i\n",
    "del train_ds\n",
    "del test_ds\n",
    "del test_tfr\n",
    "del train_tfr\n",
    "del d_path\n",
    "del dataset\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feeding tensors to a layer which calculated the Mel Spectrogram for the given audio instance\n",
    "# this in turn is treated like an image, and 2D convolutions are applied, and sent to a trained Resnet\n",
    "# the Resnet weights are frozen, and is only used to extract features from the Mel Spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NuXW0ZEkEnIL"
   },
   "outputs": [],
   "source": [
    "class LogMelSpectrogram(tf.keras.layers.Layer):\n",
    "    \"\"\"Compute log-magnitude mel-scaled spectrograms.\"\"\"\n",
    "\n",
    "    def __init__(self, sample_rate, fft_size, hop_size, n_mels,\n",
    "                 f_min=0.0, f_max=None, **kwargs):\n",
    "        super(LogMelSpectrogram, self).__init__(**kwargs)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.fft_size = fft_size\n",
    "        self.hop_size = hop_size\n",
    "        self.n_mels = n_mels\n",
    "        self.f_min = f_min\n",
    "        self.f_max = f_max if f_max else sample_rate / 2\n",
    "        self.mel_filterbank = tf.signal.linear_to_mel_weight_matrix(\n",
    "            num_mel_bins=self.n_mels,\n",
    "            num_spectrogram_bins=fft_size // 2 + 1,\n",
    "            sample_rate=self.sample_rate,\n",
    "            lower_edge_hertz=self.f_min,\n",
    "            upper_edge_hertz=self.f_max)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.non_trainable_weights.append(self.mel_filterbank)\n",
    "        super(LogMelSpectrogram, self).build(input_shape)\n",
    "\n",
    "    def call(self, waveforms):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        waveforms : tf.Tensor, shape = (None, n_samples)\n",
    "            A Batch of mono waveforms.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        log_mel_spectrograms : (tf.Tensor), shape = (None, time, freq, ch)\n",
    "            The corresponding batch of log-mel-spectrograms\n",
    "        \"\"\"\n",
    "        def _tf_log10(x):\n",
    "            numerator = tf.math.log(x)\n",
    "            denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))\n",
    "            return numerator / denominator\n",
    "\n",
    "        def power_to_db(magnitude, amin=1e-16, top_db=80.0):\n",
    "            \"\"\"\n",
    "            https://librosa.github.io/librosa/generated/librosa.core.power_to_db.html\n",
    "            \"\"\"\n",
    "            ref_value = tf.reduce_max(magnitude)\n",
    "            log_spec = 10.0 * _tf_log10(tf.maximum(amin, magnitude))\n",
    "            log_spec -= 10.0 * _tf_log10(tf.maximum(amin, ref_value))\n",
    "            log_spec = tf.maximum(log_spec, tf.reduce_max(log_spec) - top_db)\n",
    "\n",
    "            return log_spec\n",
    "\n",
    "        spectrograms = tf.signal.stft(waveforms,\n",
    "                                      frame_length=self.fft_size,\n",
    "                                      frame_step=self.hop_size,\n",
    "                                      pad_end=False)\n",
    "\n",
    "        magnitude_spectrograms = tf.abs(spectrograms)\n",
    "\n",
    "        mel_spectrograms = tf.matmul(tf.square(magnitude_spectrograms),\n",
    "                                     self.mel_filterbank)\n",
    "\n",
    "        log_mel_spectrograms = power_to_db(mel_spectrograms)\n",
    "\n",
    "        # add channel dimension\n",
    "        log_mel_spectrograms = tf.expand_dims(log_mel_spectrograms, 3)\n",
    "        return log_mel_spectrograms\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'fft_size': self.fft_size,\n",
    "            'hop_size': self.hop_size,\n",
    "            'n_mels': self.n_mels,\n",
    "            'sample_rate': self.sample_rate,\n",
    "            'f_min': self.f_min,\n",
    "            'f_max': self.f_max,\n",
    "        }\n",
    "        config.update(super(LogMelSpectrogram, self).get_config())\n",
    "\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7DztZDKZErDQ"
   },
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "\n",
    "feature_extractor_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url,\n",
    "                                         input_shape=(311, 128, 3))\n",
    "\n",
    "feature_extractor_layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3Ox4Wh1E0p3"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (BatchNormalization, Conv2D, Dense, Dropout, Flatten, Input, MaxPool2D)\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def ConvModel(n_classes, sample_rate=SAMPLE_RATE, duration=MAX_DURATION,\n",
    "              fft_size=FFT_SIZE, hop_size=HOP_SIZE, n_mels=N_MEL_BINS, fmin=F_MIN, fmax=F_MAX):\n",
    "    n_samples = sample_rate * duration\n",
    "    input_shape = (n_samples,)\n",
    "\n",
    "    x = Input(shape=input_shape, name='input', dtype='float32')    \n",
    "    y = LogMelSpectrogram(sample_rate, fft_size, hop_size, n_mels, fmin, fmax)(x)\n",
    "    y = BatchNormalization(axis=2)(y)\n",
    "\n",
    "\n",
    "    y = Conv2D(6, (3,3), padding='same')(y)  \n",
    "    y = Dropout(0.4)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = Conv2D(3, (3,3), padding='same')(y)  \n",
    "    y = Dropout(0.4)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    y = feature_extractor_layer(y, training=False)\n",
    "\n",
    "    y = Dense(1024, activation='relu')(y)\n",
    "    y = Dropout(0.4)(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    y = Dense(512, activation='relu')(y)\n",
    "    y = Dropout(0.4)(y)\n",
    "    y = Dense(n_classes, activation='softmax')(y)\n",
    "\n",
    "    return Model(inputs=x, outputs=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sU_6VQUAE9hT"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, schedules\n",
    "\n",
    "n_classes = N_CLASSES\n",
    "model = ConvModel(n_classes)\n",
    "\n",
    "lr_schedule = schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.05, decay_steps=1000, decay_rate=0.96, staircase=False\n",
    ")\n",
    "sgd = SGD(learning_rate=lr_schedule, momentum=0.85)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ESz1jZab2CTK"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = len(train)//BATCH_SIZE\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEUCrrpqOOlW"
   },
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'Model-4'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    verbose=2)\n",
    "\n",
    "model.fit(train_final, \n",
    "          epochs=50, \n",
    "          steps_per_epoch=steps_per_epoch, \n",
    "          validation_data=test_final, \n",
    "          validation_steps=2, \n",
    "         callbacks=[model_checkpoint_callback])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Model-Mel-1.ipynb",
   "provenance": [
    {
     "file_id": "1q35_3CNspVmPhRYRAt9bhen4tzxjMSkk",
     "timestamp": 1598771074726
    },
    {
     "file_id": "1LmlvpPSOxqcrLlZD011xruurhcRCyfs2",
     "timestamp": 1598698570967
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
